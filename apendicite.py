# -*- coding: utf-8 -*-
"""apendicite.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xFx5HuT0QK3ysiueqUd0WuR368CdqW99
"""

import pandas as pd
import numpy as np
from collections import Counter
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler

#normalização de dados

# carrega o CSV
df = pd.read_csv('/content/drive/MyDrive/sistemas inteligentes/apendicites.csv')

# drop nas linhas sem Diagnosis e manter as 3 classes
df = df.dropna(subset=['Diagnosis'])

# definir X e y
y = df['Diagnosis']  # mantém as 3 classes originais

# só colunas numéricas
X = df.select_dtypes(include=['float64', 'int64']).drop(columns=['US_Number'], errors='ignore')

# imputar dados faltantes
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# normalizar
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_imputed)

# dividir treino
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)

# balanceamento de dados

# três técnicas de balanceamento pra treino
balance_methods = {
    'SMOTE': SMOTE(random_state=42),
    'RandomUnderSampler': RandomUnderSampler(random_state=42),
    'ADASYN': ADASYN(random_state=42)
}

# treinamento e hiperparametrização

# três modelos diferentes e seus grids pra RandomizedSearch
models = {
    'RandomForest': {
        'model': RandomForestClassifier(random_state=42),
        'params': {
            'n_estimators': [100, 150, 200],
            'max_depth': [None, 10, 20],
            'min_samples_split': [2, 5],
            'max_features': ['sqrt', 'log2']
        }
    },
    'GradientBoosting': {
        'model': GradientBoostingClassifier(random_state=42),
        'params': {
            'n_estimators': [100, 150, 200],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [3, 5, 7]
        }
    },
    'LogisticRegression': {
        'model': LogisticRegression(random_state=42, max_iter=1000),
        'params': {
            'C': [0.1, 1, 10],
            'penalty': ['l2'],
            'solver': ['lbfgs']
        }
    }
}

# cross validation e treino

best_models = {}

print("Iniciando treino e hiperparametrização...\n")

for bal_name, balancer in balance_methods.items():
    print(f"Balanceamento: {bal_name}")
    X_bal, y_bal = balancer.fit_resample(X_train, y_train)
    print(f"Distribuição balanceada: {Counter(y_bal)}")

    for model_name, model_info in models.items():
        print(f" - Treinando modelo: {model_name}")

        search = RandomizedSearchCV(
            estimator=model_info['model'],
            param_distributions=model_info['params'],
            n_iter=5,
            cv=3,
            scoring='accuracy',
            random_state=42,
            n_jobs=-1,
            verbose=0
        )
        search.fit(X_bal, y_bal)

        best_model = search.best_estimator_

# cross-validation no treino balanceado

        cv_scores = cross_val_score(best_model, X_bal, y_bal, cv=3, scoring='accuracy')

        print(f"   Melhor params: {search.best_params_}")
        print(f"   CV Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}")

        key = f"{bal_name}_{model_name}"
        best_models[key] = best_model

    print("")

# inferência

# inferência no X_test com os 3 modelos do SMOTE
print("Inferência no conjunto de teste (usando SMOTE para exemplo):\n")

def inferencia_tripla(modelos, paciente):
    preds = []
    for nome, mod in modelos.items():
        pred = mod.predict(paciente.reshape(1, -1))[0]
        preds.append(pred)
    # exemplo de votação simples (moda)
    pred_final = max(set(preds), key=preds.count)
    return preds, pred_final

# exemplo: pegar 5 pacientes do teste e inferir
X_test_sample = X_test[:5]
y_test_sample = y_test.iloc[:5]

for i in range(len(X_test_sample)):
    paciente = X_test_sample[i]
    preds, final = inferencia_tripla({k: v for k,v in best_models.items() if 'SMOTE' in k}, paciente)
    print(f"Paciente {i+1} - Real: {y_test_sample.iloc[i]}")
    print(f"Predições individuais: {preds}")
    print(f"Predição final (votação): {final}\n")

# avaliação final

# avaliar acurácia no teste dos melhores modelos SMOTE só pra exemplo
for key, model in best_models.items():
    if 'SMOTE' in key:
        y_pred = model.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        print(f"Acurácia no teste do modelo {key}: {acc:.4f}")